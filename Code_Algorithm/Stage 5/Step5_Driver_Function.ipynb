{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Next Steps 04.02.2018\\n\\nDecision Tree:\\n- A conditional statement needs to be added to the Feature/Target objects.  \\n- If Prediction_type = Known_dataset, then use the current set up.\\n- else: \\n- X_test will need to be replaced with the features from our unknown dataset (the master docketsheet)\\n- So we'll need to drop the first and last columns.  \\n- The function should then return a dataframe with the correction predictions. \\n\\nAlso:\\n- We need to create an option in the code to include or not include the Dataframe_test.  \\n- This will need to be linked to the above decision tree conditional statement. \\n- This way we can have an option to train the model and once we get the best set of hyperparameters\\nrun the model to make a prediction on the unknown dataset. \\n\\nQuestion:\\n- How do we set up our code such that we only run stp3B when we want to?\\n\\n\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Next Steps 04.02.2018\n",
    "\n",
    "Decision Tree:\n",
    "- A conditional statement needs to be added to the Feature/Target objects.  \n",
    "- If Prediction_type = Known_dataset, then use the current set up.\n",
    "- else: \n",
    "- X_test will need to be replaced with the features from our unknown dataset (the master docketsheet)\n",
    "- So we'll need to drop the first and last columns.  \n",
    "- The function should then return a dataframe with the correction predictions. \n",
    "\n",
    "Also:\n",
    "- We need to create an option in the code to include or not include the Dataframe_test.  \n",
    "- This will need to be linked to the above decision tree conditional statement. \n",
    "- This way we can have an option to train the model and once we get the best set of hyperparameters\n",
    "run the model to make a prediction on the unknown dataset. \n",
    "\n",
    "Question:\n",
    "- How do we set up our code such that we only run stp3B when we want to?\n",
    "\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# IMPORT LIBRARIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# IMPORT MODULES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "os.chdir(r'/home/ccirelli2/Desktop/Docket-Sheet-Classification-v2/Modules')\n",
    "import Step1_Module_Ngrams_FreqDist_version4_Ngrams as stp1\n",
    "import Step2_P2_Module_Get_Top_Words_version4_Ngrams as stp2\n",
    "import Step3_Module_Ngrams_Docketsheet_KeyWord_FreqDist as stp3\n",
    "import Step4_Module_Machine_Learning_Algorithms as stp4\n",
    "import Step5_Module_Measure_DocketSheetEntries_NoMatches as stp5_nomatch\n",
    "import Step5_Module_Measure_Dependencies_Between_Stages as stp5_dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# IMPORT NGRAM RESULTS:  CALCULATION = AVERAGE APPEARANCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "os.chdir(r'/home/ccirelli2/Desktop/Docket-Sheet-Classification/All_Results/Result_Ngrams')\n",
    "Nograms_appearance = pd.read_excel(r'Docketsheet_FreqDist_Nograms_Average_appearance.xlsx')\n",
    "Bigrams_appearance = pd.read_excel(r'Docketsheet_FreqDist_Bigrams_Average_appearance.xlsx')\n",
    "Trigrams_appearance = pd.read_excel(r'Docketsheet_FreqDist_Trigrams_Average_appearance.xlsx')\n",
    "Quadgrams_appearance = pd.read_excel(r'Docketsheet_FreqDist_Quadgrams_Average_appearance.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# IMPORT NGRAM RESULTS:  CALCULATION = FREQUENCY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Nograms_frequency = pd.read_excel(r'Docketsheet_FreqDist_Nograms_Frequency_distribution.xlsx')\n",
    "Bigrams_frequency = pd.read_excel(r'Docketsheet_FreqDist_Bigrams_Frequency_distribution.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# IMPORT DOCKETSHEET WITH PRE-CLASSIFIED STAGES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "os.chdir(r'/home/ccirelli2/Desktop/Docket-Sheet-Classification/Data_Files_applicable_all_code')\n",
    "Docketsheet_train = 'DocketSheet Classification_70_02.22.2018.xlsx'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# IMPORT MASTER DOCKETSHEET WITH NON-CLASSIFIED STAGES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Only run if the document has yet to be created\n",
    "'''\n",
    "File_masterdocketsheet = r'Master_Docket_sheets.xlsx'\n",
    "Destination_loc = r'/home/ccirelli2/Desktop/Docket-Sheet-Classification/Data_Files_applicable_all_code'\n",
    "df_transformed = stp3.transform_Master_Docketsheet(File_masterdocketsheet, Write2Excel =True, \n",
    "                                               destination_location = Destination_loc)\n",
    "'''\n",
    "os.chdir(r'/home/ccirelli2/Desktop/Docket-Sheet-Classification/Stage5_Predictions_Results')\n",
    "Docketsheet_test = r'Master_Docketsheet_Chunk4.xlsx'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# CHUNK MASTER DOCKETSHEET\n",
    "def chunk_master_docketsheetfile(Excel_file):\n",
    "\n",
    "    df = pd.read_excel(Excel_file)\n",
    "    df_chunked = df.iloc[150001:200000,:]\n",
    "\n",
    "    # Write to Excel Module\n",
    "    stp4.write_to_excel(    \n",
    "                dataframe = df_chunked, \n",
    "                location = r'/home/ccirelli2/Desktop/Docket-Sheet-Classification/Stage5_Predictions_Results',\n",
    "                filename = 'Master_Docketsheet_Chunk4')\n",
    "\n",
    "    return df_chunked\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# DRIVER FUNCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def MasterFunction( TrainTestMode, # 04.03.2018: This refers to stp3A & B. \n",
    "    \n",
    "                    stp2_FreqDist_file, \n",
    "                        stp2_Calculation_meth, stp2_methodology_top_words, \n",
    "                        stp2_write2excel, stp2_destination_location, \n",
    "                        stp2_Ngram_type,\n",
    "                   \n",
    "                   stp3A_Docketsheet, \n",
    "                        stp3A_DirNgramLoc, stp3A_Iterable, \n",
    "                        stp3A_KeyPhrase, stp3A_Destination_location,\n",
    "                        stp3A_Transpose4mlModel, stp3A_Write2Excel,\n",
    "                   \n",
    "                   stp3B_Docketsheet, \n",
    "                        stp3B_DirNgramLoc, stp3B_Iterable, \n",
    "                        stp3B_KeyPhrase, stp3B_Destination_location,\n",
    "                        stp3B_Transpose4mlModel, stp3B_Write2Excel,\n",
    "                   \n",
    "                   stp4_Max_Depth, \n",
    "                        stp4_TrainTest, \n",
    "                        stp4_Metric):\n",
    "    \n",
    "    # BODY OF FUNCTION STARTS HERE:_____________________________________________________________________\n",
    "    \n",
    "    # STEP2: GENERATE KEY NGRAMS____________________________________________________:\n",
    "    \n",
    "    print('Step2: Generating our Key Ngrmas...\\n')\n",
    "    TopWords_Toggle_Calc_Methodology = stp2.get_top_words_toggle_methodology(\n",
    "                    # Features\n",
    "                    stp2_FreqDist_file, \n",
    "                    stp2_Calculation_meth, \n",
    "                    stp2_methodology_top_words, \n",
    "                    stp2_write2excel, \n",
    "                    stp2_destination_location, \n",
    "                    stp2_Ngram_type)\n",
    "    # If writing to Excel, revert back to main dir where docketsheet is located. \n",
    "    os.chdir(r'/home/ccirelli2/Desktop/Docket-Sheet-Classification/Data_Files_applicable_all_code')\n",
    "    \n",
    "    \n",
    "    # STEP3_A: DOCKETSHEET NGRAM APPEARANCE - TRAINING MODE_________________________:\n",
    "    \n",
    "    # Added 04.03.2018: Toggle Between Training & Test Modes\n",
    "    if TrainTestMode == 'Training_Mode_Known_Data':\n",
    "        print('Step3_A: Generating the Ngram matches for the \\'Training\\' Docketsheet entries...\\n')\n",
    "        stp3A_KeyNgram_Appearance_Training_DocketSheetEntries = stp3.get_DocketSheet_KeyWord_Appearance_Master(\n",
    "                    # Features\n",
    "                    stp3_Docketsheet                 = stp3A_Docketsheet, \n",
    "                    stp3_DirNgramLoc                 = stp3A_DirNgramLoc, \n",
    "                    stp3_Iterable                    = stp3A_Iterable, \n",
    "                    stp3_KeyPhrase                   = stp3A_KeyPhrase, \n",
    "                    stp3_Destination_location        = stp3A_Destination_location,\n",
    "                    stp3_Transpose4mlModel           = stp3A_Transpose4mlModel, \n",
    "                    stp3_Write2Excel                 = stp3A_Write2Excel, \n",
    "                    stp3_Target_file                 = TopWords_Toggle_Calc_Methodology)\n",
    "       \n",
    "        # Step4: Decision Tree Prediction\n",
    "        print('Step4:  Generating Decision Tree prediction\\n')\n",
    "        DecisionTreePrediction = stp4.simple_decision_tree(\n",
    "                    Max_Depth                        = stp4_Max_Depth, \n",
    "                    TrainTest                        = stp4_TrainTest, \n",
    "                    Metric                           = stp4_Metric,\n",
    "                    Docketsheet_known                = stp3A_KeyNgram_Appearance_Training_DocketSheetEntries,\n",
    "                    Docketsheet_unknown              = None, # None as it will not be used in Training mode. \n",
    "                    TrainTestMode                    = TrainTestMode) #added 04.03.2018\n",
    "    \n",
    "        # Define File Name for In-Memory Reference (Not reading from a file + path name)\n",
    "        df_inmemory_name = str(stp2_Ngram_type + '_' + stp2_Calculation_meth + '_' + \n",
    "        stp2_methodology_top_words + '_' + stp4_Metric + '_' + 'Depth' + '_' + str(stp4_Max_Depth))\n",
    "    \n",
    "        # Calculate the number of docket sheet rows without a match.  Send to user via print. \n",
    "        Count_zero = stp5_nomatch.get_count_nomatch_columns(\n",
    "            File = stp3A_KeyNgram_Appearance_Training_DocketSheetEntries, Count = 'Count_zero')\n",
    "        Count_total = stp5_nomatch.get_count_nomatch_columns(\n",
    "            File = stp3A_KeyNgram_Appearance_Training_DocketSheetEntries, Count = 'Count_all')\n",
    "    \n",
    "        # Calculate the extent of dependencies between the KeyWords in Each Stage. \n",
    "        Perct_dependency = stp5_dependencies.measure_dependencies(TopWords_Toggle_Calc_Methodology)\n",
    "    \n",
    "        # OTHER PERFORMANCE METRICS REGARDING THE MODEL\n",
    "        print('Other performance metrics:')\n",
    "        print('\\tPercent Dependency:   The Ngram Key Selection resulted in =>' + ' ' + str(Perct_dependency) + ' of overlap (dependency) between the key words')\n",
    "        print('\\tPercent None-Matches: In addition, this selection resulted in => ', round(Count_zero/Count_total, 2)*100, ' ' + \n",
    "          'percentage of Docketsheet rows without a matching Ngram', '\\n')\n",
    "    \n",
    "        return DecisionTreePrediction\n",
    "    \n",
    "    \n",
    "    # STEP3 A&B - TESTING MODE________________________________________________________________\n",
    "    \n",
    "    elif TrainTestMode == 'Prediction_Mode_Unknown_Data/':\n",
    "        print('Step3_A: Generating the Ngram matches for the \\'Training\\' Docketsheet entries...\\n')\n",
    "        stp3A_KeyNgram_Appearance_Training_DocketSheetEntries = stp3.get_DocketSheet_KeyWord_Appearance_Master(\n",
    "                    # Features\n",
    "                    stp3_Docketsheet                 = stp3A_Docketsheet, \n",
    "                    stp3_DirNgramLoc                 = stp3A_DirNgramLoc, \n",
    "                    stp3_Iterable                    = stp3A_Iterable, \n",
    "                    stp3_KeyPhrase                   = stp3A_KeyPhrase, \n",
    "                    stp3_Destination_location        = stp3A_Destination_location,\n",
    "                    stp3_Transpose4mlModel           = stp3A_Transpose4mlModel, \n",
    "                    stp3_Write2Excel                 = stp3A_Write2Excel, \n",
    "                    stp3_Target_file                 = TopWords_Toggle_Calc_Methodology)\n",
    "    \n",
    "        # Added on 03.31.2018:  Step_3 B was added. \n",
    "        print('Step3_B: Generating the Ngram matches for the \\'Test\\' Docketsheet entries...\\n')\n",
    "        stp3B_KeyNgram_Appearance_Test_DocketSheetEntries = stp3.get_DocketSheet_KeyWord_Appearance_Master(\n",
    "                    # Features\n",
    "                    stp3_Docketsheet                 = stp3B_Docketsheet, \n",
    "                    stp3_DirNgramLoc                 = stp3B_DirNgramLoc, \n",
    "                    stp3_Iterable                    = stp3B_Iterable, \n",
    "                    stp3_KeyPhrase                   = stp3B_KeyPhrase, \n",
    "                    stp3_Destination_location        = stp3B_Destination_location,\n",
    "                    stp3_Transpose4mlModel           = stp3B_Transpose4mlModel, \n",
    "                    stp3_Write2Excel                 = stp3B_Write2Excel, \n",
    "                    stp3_Target_file                 = TopWords_Toggle_Calc_Methodology)\n",
    "    \n",
    "        # Step4: Decision Tree Prediction\n",
    "        print('Step4:  Generating Decision Tree prediction\\n')\n",
    "        DecisionTreePrediction = stp4.simple_decision_tree(\n",
    "                    Max_Depth                        = stp4_Max_Depth, \n",
    "                    TrainTest                        = stp4_TrainTest, \n",
    "                    Metric                           = stp4_Metric,\n",
    "                    Docketsheet_known                = stp3A_KeyNgram_Appearance_Training_DocketSheetEntries,\n",
    "                    Docketsheet_unknown              = stp3B_KeyNgram_Appearance_Test_DocketSheetEntries,\n",
    "                    TrainTestMode                    = TrainTestMode) #added 04.03.2018\n",
    "       \n",
    "        return DecisionTreePrediction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# NOTES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nNote that you the ytrain and test objects likely contain the arrays and attributes that we are calling like confusion matrix \\nand or report are likely just ways to arrange these predictions. \\n**** Note that you created a new object in Step4, the decision tree model, that allows for you to return a dataframe of the \\nindividual predictions.  This function has dependencies, in particular the second function that generates the Accuracy, \\nMatrics, etc scores.  You need to find a way to stop the second function from running and to only pass the data frame \\nobject to the user. \\n\\n'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Note that you the ytrain and test objects likely contain the arrays and attributes that we are calling like confusion matrix \n",
    "and or report are likely just ways to arrange these predictions. \n",
    "**** Note that you created a new object in Step4, the decision tree model, that allows for you to return a dataframe of the \n",
    "individual predictions.  This function has dependencies, in particular the second function that generates the Accuracy, \n",
    "Matrics, etc scores.  You need to find a way to stop the second function from running and to only pass the data frame \n",
    "object to the user. \n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# RUN FUNCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step2: Generating our Key Ngrmas...\n",
      "\n",
      "Step3_A: Generating the Ngram matches for the 'Training' Docketsheet entries...\n",
      "\n",
      "\t25% completed\n",
      "\t50% completed\n",
      "\t75% completed\n",
      "\n",
      "Step3_B: Generating the Ngram matches for the 'Test' Docketsheet entries...\n",
      "\n",
      "\t25% completed\n",
      "\t50% completed\n",
      "\t75% completed\n",
      "\n",
      "Step4:  Generating Decision Tree prediction\n",
      "\n",
      "Generating prediction for the unknown dataset\n",
      "\n",
      "Returning prediction for the unknown dataset: \n",
      "\n"
     ]
    }
   ],
   "source": [
    "KeyNgrams = MasterFunction(\n",
    "                    \n",
    "                    TrainTestMode                  = 'Prediction_Mode_Unknown_Data',\n",
    "                        \n",
    "                    stp2_FreqDist_file             = Nograms_frequency,             # In general Freq seems to perf better. \n",
    "                        stp2_Calculation_meth      = 'CalculationI_homebrew_STDV', \n",
    "                        stp2_methodology_top_words = 'Top15_highest_STDV', \n",
    "                        stp2_write2excel           = False, \n",
    "                        stp2_destination_location  = '/home/ccirelli2/Desktop/', \n",
    "                        stp2_Ngram_type            = 'Nograms', \n",
    "\n",
    "                    stp3A_Docketsheet               = Docketsheet_train, \n",
    "                        stp3A_DirNgramLoc           = None, \n",
    "                        stp3A_Iterable              = False, \n",
    "                        stp3A_KeyPhrase             = None, \n",
    "                        stp3A_Destination_location  = None,\n",
    "                        stp3A_Transpose4mlModel     = True, \n",
    "                        stp3A_Write2Excel           = False, \n",
    "\n",
    "                    stp3B_Docketsheet               = Docketsheet_test, \n",
    "                        stp3B_DirNgramLoc           = None, \n",
    "                        stp3B_Iterable              = False, \n",
    "                        stp3B_KeyPhrase             = None, \n",
    "                        stp3B_Destination_location  = None,\n",
    "                        stp3B_Transpose4mlModel     = True, \n",
    "                        stp3B_Write2Excel           = False,\n",
    "    \n",
    "                    stp4_Max_Depth                  = 13, \n",
    "                        stp4_TrainTest              = 'Train', \n",
    "                        stp4_Metric                 = 'Accuracy')              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 5.,  5.,  5., ...,  1.,  1.,  1.])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "KeyNgrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Append Results to File "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a Dataframe from the predictions\n",
    "\n",
    "def create_dataframe_write2Excel(Array, Docketsheet_test):\n",
    "    \n",
    "    os.chdir(r'/home/ccirelli2/Desktop/Docket-Sheet-Classification/Stage5_Predictions_Results')\n",
    "    \n",
    "    # Read the Docketsheet Test file into memory as a df\n",
    "    df_docketsheet = pd.read_excel(Docketsheet_test)\n",
    "    # Limit dataframe to match what was input into the ML model. \n",
    "    delimiter = [isinstance(x,str) for x in df_docketsheet['docket_text']]\n",
    "    df_limited = df_docketsheet[delimiter]\n",
    "    df_limited['Prediction'] = Array\n",
    "    \n",
    "    print(df_limited.head())\n",
    "    stp4.write_to_excel(dataframe = df_limited, \n",
    "                    location = r'/home/ccirelli2/Desktop/Docket-Sheet-Classification/Stage5_Predictions_Results',\n",
    "                    filename = 'Step5_Prediction_Appended_Docketsheet_Chunk4')\n",
    "    \n",
    "    return None\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ccirelli2/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  if sys.path[0] == '':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              case_number activity_date activity_number  \\\n",
      "150001  1:15-cv-01452-WBH    2016-02-05              55   \n",
      "150002  1:15-cv-01452-WBH    2016-02-09              56   \n",
      "150003  1:15-cv-01452-WBH    2016-02-18              57   \n",
      "150004  1:15-cv-01452-WBH    2016-02-23              58   \n",
      "150005  1:15-cv-01452-WBH    2016-03-03              59   \n",
      "\n",
      "                                              docket_text  document_try_flag  \\\n",
      "150001  NOTICE of Appearance by Steven Wright Likens o...                  0   \n",
      "150002  MOTION for Application of the Court's Recent R...                  0   \n",
      "150003  RESPONSE in Opposition re MOTION for Applicati...                  0   \n",
      "150004  REPLY BRIEF re (54 in 1:15-cv-02386-WBH) MOTIO...                  0   \n",
      "150005  NOTICE to Take Deposition of Gentiva 30(b)(6) ...                  0   \n",
      "\n",
      "        document_flag  document_link document_text state  district  \\\n",
      "150001              0            NaN           NaN    GA  Northern   \n",
      "150002              0            NaN           NaN    GA  Northern   \n",
      "150003              0            NaN           NaN    GA  Northern   \n",
      "150004              0            NaN           NaN    GA  Northern   \n",
      "150005              0            NaN           NaN    GA  Northern   \n",
      "\n",
      "        row_number  Prediction  \n",
      "150001          57         5.0  \n",
      "150002          58         5.0  \n",
      "150003          59         5.0  \n",
      "150004          62         5.0  \n",
      "150005          64         5.0  \n"
     ]
    }
   ],
   "source": [
    "df_prediction = create_dataframe_write2Excel(KeyNgrams, Docketsheet_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def transform_rowlvl_2_caselvl(File):\n",
    "    # Change directory\n",
    "    os.chdir(r'/home/ccirelli2/Desktop/Docket-Sheet-Classification-v2/Stage5_Predictions_Results')\n",
    "    \n",
    "    # Read File into memory as a dataframe\n",
    "    df = pd.read_excel(File)\n",
    "    # Delimit dataframe\n",
    "    df_limited = df[['case_number', 'Prediction', 'activity_date']]\n",
    "    \n",
    "    # Create a Dictionary, Key = Case Number, Value = Stage. \n",
    "    Dict = {}\n",
    "    # Create a counter to identify the first row of our dataframe. \n",
    "    Count = 0\n",
    "    # Create a list to capture the stages per case. \n",
    "    List = []\n",
    "    # Iterate over each row of the dataframe. \n",
    "    for x,y in zip(df_limited.itertuples(), df_limited[1:].itertuples()):\n",
    "        # Add to our counter over each iteration. \n",
    "        Count += 1\n",
    "        # If Count == 1, append to the list the first row's prediction value. \n",
    "        if Count == 1:\n",
    "            List.append(x[2])\n",
    "        # If count > 1\n",
    "        else:\n",
    "            # If the second row case num is equal to the first row case num, keep appending the values. \n",
    "            if y[1] == x[1]:\n",
    "                List.append(y[2])\n",
    "            # At which point the next rows case number does not equal the preceding one, append the List\n",
    "            else:\n",
    "                # Create our dictiony key == case number and value == List of stages for the given case. \n",
    "                Dict[x[1]] = set(List)\n",
    "                List = []\n",
    "      \n",
    "    # Transform Dictionary \n",
    "    Dict_new = {}\n",
    "    for key in Dict.keys():\n",
    "        List = []\n",
    "        for num in range(1,12):\n",
    "            if num in Dict[key]:\n",
    "                List.append(1)\n",
    "            else:\n",
    "                List.append(0)\n",
    "        Dict_new[key] = List\n",
    "    \n",
    "    # Create Dataframe\n",
    "    df_final = pd.DataFrame(Dict_new).transpose()\n",
    "       \n",
    "    df_final = df_final.rename(index =str, columns = {0:\"Stage1\", \n",
    "                                                      1:\"Stage2\", \n",
    "                                                      2:\"Stage3\", \n",
    "                                                      3: \"Stage4\", \n",
    "                                                      4:\"Stage5\", \n",
    "                                                      5:\"Stage6\", \n",
    "                                                      6:\"Stage7\", \n",
    "                                                      7:\"Stage8\", \n",
    "                                                      8:\"Stage9\", \n",
    "                                                      9:\"Stage10\", \n",
    "                                                      10:\"Stage11\"})\n",
    "    print(df_final.head())\n",
    "    \n",
    "    stp4.write_to_excel(dataframe = df_final, \n",
    "                    location = r'/home/ccirelli2/Desktop',\n",
    "                    filename = 'Case_Level_Predictions_Chunk1')\n",
    "    \n",
    "    # Return the dictonary to the user\n",
    "    return None\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   Stage1  Stage2  Stage3  Stage4  Stage5  Stage6  Stage7  \\\n",
      "1:10-cv-00002-ODE       1       1       1       0       1       0       0   \n",
      "1:10-cv-00007-JEC       0       1       0       1       1       1       1   \n",
      "1:10-cv-00012-TWT       1       0       1       1       1       0       1   \n",
      "1:10-cv-00017-JOF       0       1       1       1       1       1       0   \n",
      "1:10-cv-00018-RLV       0       1       1       1       1       0       0   \n",
      "\n",
      "                   Stage8  Stage9  Stage10  Stage11  \n",
      "1:10-cv-00002-ODE       1       0        0        1  \n",
      "1:10-cv-00007-JEC       0       0        0        1  \n",
      "1:10-cv-00012-TWT       0       0        0        1  \n",
      "1:10-cv-00017-JOF       1       0        0        1  \n",
      "1:10-cv-00018-RLV       1       0        0        1  \n"
     ]
    }
   ],
   "source": [
    "df_transformed = transform_rowlvl_2_caselvl(r'Step5_Prediction_Appended_Docketsheet_Chunk1.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "{'1:15-cv-01452-WBH': {(5, Timestamp('2016-03-01 00:00:00')), \n",
    "                       (5, Timestamp('2016-02-18 00:00:00')), \n",
    "                       (4, Timestamp('2016-03-16 00:00:00')), \n",
    "                       (5, Timestamp('2016-03-03 00:00:00')), \n",
    "                       (7, Timestamp('2016-03-23 00:00:00')), \n",
    "                       (5, Timestamp('2016-04-29 00:00:00')), \n",
    "                       (5, Timestamp('2016-02-23 00:00:00')), \n",
    "                       (8, Timestamp('2015-07-20 00:00:00')), \n",
    "                       (5, Timestamp('2016-03-23 00:00:00')), \n",
    "                       (5, Timestamp('2016-02-05 00:00:00')), \n",
    "                       (5, Timestamp('2015-11-18 00:00:00')), \n",
    "                       (5, Timestamp('2016-03-14 00:00:00')), \n",
    "                       (4, Timestamp('2017-06-22 00:00:00')), \n",
    "                       (5, Timestamp('2016-03-08 00:00:00')), \n",
    "                       (8, Timestamp('2017-06-08 00:00:00')), \n",
    "                       (5, Timestamp('2016-03-15 00:00:00')), \n",
    "                       (5, Timestamp('2015-06-30 00:00:00')), \n",
    "                       (11, Timestamp('2017-06-22 00:00:00')), \n",
    "                       (8, Timestamp('2017-06-07 00:00:00')), \n",
    "                       (5, Timestamp('2016-03-10 00:00:00'))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
